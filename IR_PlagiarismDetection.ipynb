{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-PlagiarismDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJongstra/Information_Retrieval_Assignment_3/blob/main/IR_PlagiarismDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXxn7diu_iP"
      },
      "source": [
        "# 1. Setup\r\n",
        "- Import all needed libraries\r\n",
        "- Google Drive mount\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLlGonAdb7yF"
      },
      "source": [
        "from google.cloud import storage\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!pip install mmh3\r\n",
        "!pip install snapy\r\n",
        "!pip install xxhash\r\n",
        "!pip install Random-Word-Generator\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import string, re, random, xxhash, time\r\n",
        "from snapy import MinHash, LSH\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_B70lpvJWF"
      },
      "source": [
        "# 2. Similarity Analysis: Ground Truth\r\n",
        "Preprocessing of a document\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZ0T12hHiEV"
      },
      "source": [
        "# \"We don't need to use a library, great!\" -> [\"we\", \"do\", \"not\", \"need\", \"to\", \"use\", \"a\", \"library\", \"great\"]\r\n",
        "def preprocess_document(document: str):\r\n",
        "    doc = document.lower()  # lower case\r\n",
        "    doc = doc.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\", \"\")  # rewrite contractions\r\n",
        "    doc = re.sub(\" [^ ]*&amp[^ ]*\", \"\", doc)  # remove random \"&amp\" in text\r\n",
        "    doc = doc.translate(str.maketrans('', '', string.digits))  # remove numbers?\r\n",
        "    doc = re.sub(\" +\", \" \", doc)  # remove double spaces\r\n",
        "    doc = doc.translate(str.maketrans('', '', string.punctuation))  # remove ALL punctuation\r\n",
        "    return doc.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJQtjrXJb4t"
      },
      "source": [
        "Load the small article dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vU3FzItcJpb"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv', index_col=0)\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU1BIa83fbwe"
      },
      "source": [
        "df['article'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFzLFWmykTIf"
      },
      "source": [
        "All the articles in the small article dataset will be processed to a list of the terms in the articles. The words are lowercased and duplicates are removed by using a set (because order does not matter in this part of the analysis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-pvBwR-ij75"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for _, row in df.iterrows():\r\n",
        "    terms = preprocess_document(row['article'])\r\n",
        "    articleList.append(set(terms))\r\n",
        "    \r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZ5tTpEvSrv"
      },
      "source": [
        "Calculate the jaccard index between each two documents in the data set by dividing the length of the intersection with the length of the union of the two sets. Save the values to a list to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrEpQqXnT_1"
      },
      "source": [
        "jaccardVals = []\r\n",
        "\r\n",
        "for doc1idx in range(len(articleList)):\r\n",
        "  doc1 = articleList[doc1idx]\r\n",
        "  doc2idx = doc1idx + 1\r\n",
        "  while doc2idx < len(articleList):\r\n",
        "    doc2 = articleList[doc2idx]\r\n",
        "    jaccard = len(doc1.intersection(doc2)) / len(doc1.union(doc2))\r\n",
        "    jaccardVals.append(jaccard)\r\n",
        "    doc2idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11mRB8VGvpo_"
      },
      "source": [
        "Plot the amount of values per bin, using a total of 50 bins.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v329XPSZrypo"
      },
      "source": [
        "jaccardVals = np.array(jaccardVals)\r\n",
        "sns.histplot(jaccardVals, bins=50)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrn3fKmv2YQ"
      },
      "source": [
        "The previous graph showed a peak in a small range of the possible similarities. To see the distribution in other ranges, we leave the peak values out.\r\n",
        "\r\n",
        "From this it is clear that there are also values in the higher ranges, however there are not a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPBpOnk4uifi"
      },
      "source": [
        "sns.histplot(jaccardVals[jaccardVals>0.2], bins=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbWLY700KaC"
      },
      "source": [
        "# 3. LSH Implementation\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJbKpQtYWeVq"
      },
      "source": [
        "## 3.1 Hash functions\r\n",
        "The class HashFunction uses the **xxhash** library to hash previously hashed shingles (64 bit) from sketches. We use a fixed size key to create different hash functions h_0 to h_|M|. Even before reading our documents, we will generate a list of these hash functions based on a **seed**.\r\n",
        "\r\n",
        "The class can also be used to hash shingles (list of strings) to a 64 bit value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAEW9rie0a_Q"
      },
      "source": [
        "# Hash function object that can be prepared\r\n",
        "# If no salt is provided this function will be a normal xxhash\r\n",
        "# If two hash functions use the same salt, they will also generate the same output for a given input\r\n",
        "# We call it salt, as it is not secret, but deterministic\r\n",
        "class HashFunction:\r\n",
        "    def __init__(self, salt: int = None):\r\n",
        "        self.salt = self.int_to_bytes(salt) if salt else b''  # store key\r\n",
        "\r\n",
        "    # [\"rose\", \"is\", \"a\"] -> 189939623769124324\r\n",
        "    def compute_strings(self, shingle: []):\r\n",
        "        h = xxhash.xxh64() # no salt needed\r\n",
        "        for word in shingle:\r\n",
        "            h.update(word)\r\n",
        "        return self.to_64_bit(h.digest())\r\n",
        "\r\n",
        "    # (hashed shingle) 189939623769124324 ->  (rank) 134237347983861913\r\n",
        "    def compute_int64(self, shingle: int):\r\n",
        "        h = xxhash.xxh64(self.salt)\r\n",
        "        h.update(self.int_to_bytes(shingle))\r\n",
        "        return self.to_64_bit(h.digest())\r\n",
        "\r\n",
        "    # convert 64 bit integer to 8 bytes\r\n",
        "    def int_to_bytes(self, i: int):\r\n",
        "        return int.to_bytes(i, length=8, byteorder='big', signed=False)\r\n",
        "\r\n",
        "    # convert 16 byte hash digest (128 bit) to a 64 bit integer (8 bytes)\r\n",
        "    def to_64_bit(self, digest: bytes):\r\n",
        "        return int.from_bytes(digest[:8], byteorder='big', signed=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-WygPQc5A8z"
      },
      "source": [
        "## 3.2 LSH functionality\r\n",
        "\r\n",
        "After all documents are added, the set is static and no documents can be added after. However, we can compare new content to our existing set using query_content(doc, s).\r\n",
        "We could implement insertion, but we won't need it for this assignment.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycDcFOrfV5_v"
      },
      "source": [
        "# Basic functionality that the library as wel as our own implementation must handle\r\n",
        "class LSHFunctionality:\r\n",
        "    def __init__(self, n_gram, bands, rows, seed):\r\n",
        "        self.n_gram = n_gram\r\n",
        "        self.bands = bands\r\n",
        "        self.rows = rows\r\n",
        "        self.seed = seed\r\n",
        "        self.signature_length = bands * rows\r\n",
        "        self.original_documents = []\r\n",
        "\r\n",
        "    # read directly from csv file\r\n",
        "    def read_csv(self, csv_file: str):\r\n",
        "        for _, row in pd.read_csv(csv_file, index_col=0).iterrows():\r\n",
        "            self.original_documents.append(row['article'])\r\n",
        "\r\n",
        "\r\n",
        "    # add documents as a list of strings\r\n",
        "    def add_documents(self, documents: []):\r\n",
        "        self.original_documents = documents\r\n",
        "\r\n",
        "    # after adding documents, call compute to start the LSH\r\n",
        "    def compute(self):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # return all similarities >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # compare all docs to 'content' and return all where >= s\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgkYp2pEV-iM"
      },
      "source": [
        "## 3.3 Using a Library\r\n",
        "We first start with implementing this functionality with the library\r\n",
        "\r\n",
        "https://pypi.org/project/snapy/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTSjafm5WQS8"
      },
      "source": [
        "class LSHLibrary(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.lsh = None\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        self.lsh = LSH(\r\n",
        "            MinHash(\r\n",
        "                self.original_documents,\r\n",
        "                n_gram=self.n_gram,\r\n",
        "                n_gram_type='term',\r\n",
        "                permutations=self.signature_length,\r\n",
        "                seed=self.seed\r\n",
        "            ),\r\n",
        "            range(len(self.original_documents)),\r\n",
        "            no_of_bands=self.bands\r\n",
        "        )\r\n",
        "\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        return self.lsh.edge_list(min_jaccard=s, jaccard_weighted=True)\r\n",
        "\r\n",
        "    # to query some content, we first have to add it to our set, minhash it and than query its id..\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        doc_id = len(self.original_documents)\r\n",
        "        self.original_documents.append(content)\r\n",
        "\r\n",
        "        # add to set (M)\r\n",
        "        self.lsh.update(MinHash(\r\n",
        "            [content],\r\n",
        "            n_gram=self.n_gram,\r\n",
        "            n_gram_type='term',\r\n",
        "            permutations=self.signature_length,\r\n",
        "            seed=self.seed\r\n",
        "        ), [doc_id])\r\n",
        "\r\n",
        "        # query matching documents\r\n",
        "        return self.lsh.query(doc_id, min_jaccard=s)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTmCAruWsm_"
      },
      "source": [
        "## 3.4 Our own implementation of LSH\r\n",
        "Our own implementation requires some additional methods to get all the functionality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zhT_B3s5gpK"
      },
      "source": [
        "class LSHImplementation(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed=123):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.hash_tables = []  # a dictionary for each band\r\n",
        "        self.M = []  # a signature for each document\r\n",
        "        self.similarities = {}  # keys are document pairs and the values are band hits\r\n",
        "        random.seed(seed) # prepare signature hash functions based on seed\r\n",
        "        self.prepared_hash_functions = [HashFunction(salt=random.getrandbits(64)) for _ in range(self.signature_length)]\r\n",
        "\r\n",
        "    # Construct M, create hash tables and compute similarities\r\n",
        "    def compute(self):\r\n",
        "        self.M = self.construct_M()\r\n",
        "        self.hash_tables = self.construct_hash_tables()\r\n",
        "        self.similarities = self.construct_similarities()\r\n",
        "\r\n",
        "    # Create a signature for each document\r\n",
        "    def construct_M(self):\r\n",
        "        M = []\r\n",
        "        for original_doc in self.original_documents:\r\n",
        "            signature = self.doc_to_signature(original_doc)\r\n",
        "            M.append(signature)\r\n",
        "        return M\r\n",
        "\r\n",
        "    # Pre process the document, shingle its contents, hash the shingles and create the signature using minhash\r\n",
        "    def doc_to_signature(self, original_doc):\r\n",
        "        # [\"rose\", \"is\", \"a\", \"rose\", \"is\", \"a\", \"rose\"]\r\n",
        "        terms = preprocess_document(original_doc)\r\n",
        "        # To set of shingles: {34, 727, 1, .., 934}\r\n",
        "        hashed_shingles = self.terms_to_hashed_shingles(terms)\r\n",
        "        signature = []\r\n",
        "        for hash_f in self.prepared_hash_functions:\r\n",
        "            # returns shingle for which h_i outputs the lowest value\r\n",
        "            min_hash = min(hashed_shingles, key=hash_f.compute_int64)\r\n",
        "            signature.append(min_hash)\r\n",
        "        return signature  # <- sketch!\r\n",
        "\r\n",
        "    # -> \"rose is a rose is a rose\"\r\n",
        "    # -> [[\"rose\", \"is\", \"a\"], [\"is\", \"a\", \"rose\"], [\"a\", \"rose\", \"is\"], [\"rose\", \"is\", \"a\"], [\"is\", \"a\", \"rose\"]]\r\n",
        "    # -> [44, 24, 17, 44, 24]\r\n",
        "    # -> {44, 24, 17}\r\n",
        "    def terms_to_hashed_shingles(self, terms):\r\n",
        "        hash_f = HashFunction()  # no salt\r\n",
        "        no_shingles = len(terms) - self.n_gram + 1\r\n",
        "        return set([hash_f.compute_strings(terms[i:i + self.n_gram]) for i in range(no_shingles)])\r\n",
        "\r\n",
        "    # Construct a hash table (dictionary) for each band, the row values in the signature is a key in the table\r\n",
        "    # If doc1 has values (1,2,3) for band 2, and doc2 also has values (1,2,3) for band 2,\r\n",
        "    # then they will end up in the same bucket.\r\n",
        "    def construct_hash_tables(self):\r\n",
        "        bands_hash_tables = []\r\n",
        "        for b in range(self.bands):\r\n",
        "            hash_table = {}\r\n",
        "            for doc_id in range(len(self.M)):\r\n",
        "                signature = self.M[doc_id]\r\n",
        "                key = tuple(signature[b * self.rows:(b + 1) * self.rows])\r\n",
        "                if key in hash_table:\r\n",
        "                    hash_table[key].append(doc_id)\r\n",
        "                else:\r\n",
        "                    hash_table[key] = [doc_id]\r\n",
        "            bands_hash_tables.append(hash_table)\r\n",
        "        return bands_hash_tables\r\n",
        "\r\n",
        "    # Construct all similarities by keeping track of all hits between documents\r\n",
        "    # Result -> {(doc1, doc2):5, (doc2, doc7):3}\r\n",
        "    # If total_bands=10, then the jaccard for doc1&2 is 5/10 = 0.5\r\n",
        "    def construct_similarities(self):\r\n",
        "        similarities = {}\r\n",
        "        for b in range(self.bands):\r\n",
        "            for bucket in self.hash_tables[b].values():\r\n",
        "                no_docs = len(bucket)\r\n",
        "                # need at least 2 docs in a bucket to have a candidate pair\r\n",
        "                if no_docs > 1:\r\n",
        "                    # make all combinations between documents in bucket d(d-1)/2\r\n",
        "                    for i in range(no_docs - 1):\r\n",
        "                        for j in range(i + 1, no_docs):\r\n",
        "                            candidate_pair = tuple([bucket[i], bucket[j]])\r\n",
        "                            if candidate_pair in similarities:\r\n",
        "                                similarities[candidate_pair] += 1\r\n",
        "                            else:\r\n",
        "                                similarities[candidate_pair] = 1\r\n",
        "        return similarities\r\n",
        "\r\n",
        "    # Get all document id's where the jaccard >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        # Now the jaccard value is the amount of band hits / total_bands, but only return if >= s\r\n",
        "        return [(doc1, doc2, hits / self.bands)\r\n",
        "                for ((doc1, doc2), hits) in self.similarities.items() if hits / self.bands >= s]\r\n",
        "\r\n",
        "    # Create a signature for the new document, and compare its bands with the bands hash table to find similar documents\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        similarities = {}\r\n",
        "        signature = self.doc_to_signature(content)\r\n",
        "        for b in range(self.bands):\r\n",
        "            candidate_pair = tuple(signature[b * self.rows:(b + 1) * self.rows])\r\n",
        "            if candidate_pair in self.hash_tables[b]:\r\n",
        "                # all documents that share the same row values in band b\r\n",
        "                for doc_id in self.hash_tables[b][candidate_pair]:\r\n",
        "                    # keep counters how many times another doc has the same band values\r\n",
        "                    if doc_id in similarities:\r\n",
        "                        similarities[doc_id] += 1\r\n",
        "                    else:\r\n",
        "                        similarities[doc_id] = 1\r\n",
        "\r\n",
        "        # Now the jaccard value is the amount of band hits / total_bands, but only return if >= s\r\n",
        "        return [(doc, hits / self.bands)\r\n",
        "                for (doc, hits) in similarities.items() if hits / self.bands >= s]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wf4yzEhZ9uJ"
      },
      "source": [
        "## 3.5 Simple Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YaW2HL9aZcg"
      },
      "source": [
        "# example from https://pypi.org/project/snapy/\r\n",
        "documents = [\r\n",
        "  'Jupiter is primarily composed of hydrogen and a quarter of its mass being helium',\r\n",
        "  'Jupiter moving out of the inner Solar System would have allowed the formation of inner planets.',\r\n",
        "  'A helium atom has about four times as much mass as a hydrogen atom, so the composition changes when described as the proportion of mass contributed by different atoms.',\r\n",
        "  'Jupiter is primarily composed of hydrogen and a quarter of its mass being helium',\r\n",
        "  'A helium atom has about four times as much mass as a hydrogen atom and the composition changes when described as a proportion of mass contributed by different atoms.',\r\n",
        "  'Theoretical models indicate that if Jupiter had much more mass than it does at present, it would shrink.',\r\n",
        "  'This process causes Jupiter to shrink by about 2 cm each year.',\r\n",
        "  'Jupiter is mostly composed of hydrogen with a quarter of its mass being helium',\r\n",
        "  'The Great Red Spot is large enough to accommodate Earth within its boundaries.'\r\n",
        "]\r\n",
        "# changed 'much' to 'a lot' from document 5\r\n",
        "plagiarized_doc = 'Theoretical models indicate that if Jupiter had a lot more mass than it does at present, it would shrink.'\r\n",
        "\r\n",
        "# test both implementations\r\n",
        "for constructor in [LSHImplementation, LSHLibrary]:\r\n",
        "  lsh = constructor(n_gram=2, bands=10, rows=2, seed=999)\r\n",
        "  lsh.add_documents(documents)\r\n",
        "  lsh.compute()\r\n",
        "  s = 0.4\r\n",
        "  print(f\"\\n========== {lsh.__class__.__name__} ==========\")\r\n",
        "  print(f\"All similarities s>={s}:\", lsh.get_all_similarities(s=s))\r\n",
        "  print(f\"Find similar documents to plagiarized doc with s>={s} (doc 5 expected):\", lsh.query_content(plagiarized_doc, s=s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvUqEmqFdpXY"
      },
      "source": [
        "## 3.6 Time comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhrf71NEdxI6"
      },
      "source": [
        "print(df.head())\r\n",
        "\r\n",
        "for constructor in [LSHImplementation, LSHLibrary]:\r\n",
        "  lsh = constructor(n_gram=4, bands=20, rows=4, seed=17)\r\n",
        "\r\n",
        "  print(f\"\\n========== {lsh.__class__.__name__} ==========\")\r\n",
        "  print(\"Read CSV.. \", end='')\r\n",
        "  time_start = time.time()\r\n",
        "  lsh.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv')\r\n",
        "  print(f\"({round((time.time()-time_start)/60, 2)} minutes)\")\r\n",
        "\r\n",
        "  print(\"Construct M.. \", end='')\r\n",
        "  time_start = time.time()\r\n",
        "  lsh.compute()\r\n",
        "  print(f\"({round((time.time() - time_start) / 60, 2)} minutes)\")\r\n",
        "\r\n",
        "  s = 0.6\r\n",
        "  print(f\"Find all similar documents with s >= {s}\")\r\n",
        "  time_start = time.time()\r\n",
        "  sim = lsh.get_all_similarities(s=s)\r\n",
        "  print(f\"{len(sim)} similarities found ({round((time.time() - time_start) / 60, 2)} minutes): \", sim)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytzOUepjsAL"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm7K4FJUfroQ"
      },
      "source": [
        "### Prepare Some Plagiarised Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkA8j9JshPOS"
      },
      "source": [
        "from RandomWordGenerator import RandomWord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq6OzbByrE1e"
      },
      "source": [
        "# Replace first x words by random words\n",
        "def create_plagiarised_doc_range(document, x):\n",
        "  words=doc_1.split(\" \")\n",
        "  rw = RandomWord(max_word_size = 5,\n",
        "                constant_word_size=True,\n",
        "                include_digits=False,\n",
        "                special_chars=r\"@_!#$%^&*()<>?/\\|}{~:\",\n",
        "                include_special_chars=False)\n",
        "  for word in range(0, x):\n",
        "    words[word] = rw.generate()\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Replace every xth word by a random word\n",
        "def create_plagiarised_doc_step(document, x):\n",
        "  words=doc_1.split(\" \")\n",
        "  rw = RandomWord(max_word_size = 5,\n",
        "                constant_word_size=True,\n",
        "                include_digits=False,\n",
        "                special_chars=r\"@_!#$%^&*()<>?/\\|}{~:\",\n",
        "                include_special_chars=False)\n",
        "  for word in range(0, len(words), x):\n",
        "    words[word] = rw.generate()\n",
        "  return \" \".join(words)\n",
        "\n",
        "\n",
        "def create_plagiarised_docs(document, duplicates):\n",
        "  # Add docstring\n",
        "  duplicates_dict = {}\n",
        "  for i in range(1, duplicates+1):\n",
        "    plagiarised_doc = create_plagiarised_doc_step(doc_1, i+1)\n",
        "    duplicates_dict[f\"plagiarised_doc_step_{i}\"] = plagiarised_doc\n",
        "  for i in range(1, duplicates+1):\n",
        "    plagiarised_doc = create_plagiarised_doc_range(doc_1, i*15)\n",
        "    duplicates_dict[f\"plagiarised_doc_range_{i}\"] = plagiarised_doc\n",
        "  return duplicates_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SVwHYnjk981"
      },
      "source": [
        "doc_1 = df['article'].iloc[0]\n",
        "doc_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5U238EFscRC"
      },
      "source": [
        "duplicates_dict = create_plagiarised_docs(doc_1, 10)\n",
        "duplicates_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-klU77dlbAj"
      },
      "source": [
        "### Calculate Jaccard Similarity between Plagiarised Documents and Original Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOg9Usl2ucTQ"
      },
      "source": [
        "# Preprocess doc_1\n",
        "doc_1_set= set(preprocess_document(doc_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uglr7PomJkU"
      },
      "source": [
        "# Calculate Jaccard Similarity between Doc_1 and its duplicates\n",
        "jaccardVals = {}\n",
        "for key in duplicates_dict:\n",
        "  duplicate_terms = preprocess_document(duplicates_dict[key])\n",
        "  duplicate_set = set(duplicate_terms)\n",
        "  jaccard = len(doc_1_set.intersection(duplicate_set)) / len(doc_1_set.union(duplicate_set))\n",
        "  jaccardVals[key] = jaccard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G069f2lanJS8"
      },
      "source": [
        "# Jaccard similarity between duplicates and document 1:\n",
        "jaccardVals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxtdhwBqmONM"
      },
      "source": [
        "jaccardVals_arr = np.array(list(jaccardVals.values()))\n",
        "sns.histplot(jaccardVals_arr, bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrfwFEx7RYC8"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvAJjAOplSM5"
      },
      "source": [
        "# constructor\n",
        "lsh = LSHImplementation(n_gram=2, bands=100, rows=3, seed=17)\n",
        "\n",
        "# add documents to set (do not pre process)\n",
        "# lsh.add_documents(articleList)\n",
        "\n",
        "# or read from csv\n",
        "# lsh.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv')\n",
        "lsh.add_documents([doc_1]) # is faster\n",
        "\n",
        "# create M and buckets\n",
        "lsh.compute() \n",
        "\n",
        "# get all similarities in database\n",
        "matches = lsh.get_all_similarities(s=0.4)\n",
        "# get matches based on new doc\n",
        "for key in duplicates_dict:\n",
        "  print(key)\n",
        "  print(duplicates_dict[key])\n",
        "  matches = lsh.query_content(duplicates_dict[key], s=0.4)\n",
        "  print(matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSsg9nTLRXlX"
      },
      "source": [
        "# set value of M\n",
        "# set value of s\n",
        "for M in range(0, 1000, 100):\n",
        "  for s in range(0, 100, 10):\n",
        "    s = s//10\n",
        "\n",
        "# Calculate precision\n",
        "\n",
        "# Create precision plot"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}