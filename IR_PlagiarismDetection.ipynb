{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-PlagiarismDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJongstra/Information_Retrieval_Assignment_3/blob/main/IR_PlagiarismDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXxn7diu_iP"
      },
      "source": [
        "# Setup\r\n",
        "- Import all needed libraries\r\n",
        "- Google Drive mount\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLlGonAdb7yF"
      },
      "source": [
        "!pip install mmh3\r\n",
        "!pip install snapy\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import string, re\r\n",
        "from snapy import MinHash, LSH\r\n",
        "from google.cloud import storage\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv6-qQBwgnDs"
      },
      "source": [
        "# Plagiarism Detection super class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK59oJ_jgKIt"
      },
      "source": [
        "class PlagiarismDetector:\r\n",
        "    def __init__(self, n_gram, bands, rows):\r\n",
        "        self.n_gram = n_gram\r\n",
        "        self.bands = bands\r\n",
        "        self.rows = rows\r\n",
        "        self.signature_length = bands * rows\r\n",
        "        self.articles = []\r\n",
        "        self.M = None\r\n",
        "\r\n",
        "    def read_articles(self, csv_file):\r\n",
        "        for _, row in csv_file.iterrows():\r\n",
        "            self.add_article(row['article'])\r\n",
        "\r\n",
        "    def add_article(self, article):\r\n",
        "        self.articles.append(self.preprocess_article(article))\r\n",
        "\r\n",
        "    def preprocess_article(self, article):\r\n",
        "        return article  # no preprocessing\r\n",
        "\r\n",
        "    def construct_M(self):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "\r\n",
        "class PlagiarismDetectorLib(PlagiarismDetector):\r\n",
        "    def __init__(self, n_gram, bands, rows):\r\n",
        "        super().__init__(n_gram, bands, rows)\r\n",
        "\r\n",
        "    def construct_M(self):\r\n",
        "        self.M = LSH(\r\n",
        "            MinHash(\r\n",
        "                self.articles,\r\n",
        "                n_gram=self.n_gram,\r\n",
        "                n_gram_type='term',\r\n",
        "                permutations=self.signature_length\r\n",
        "            ),\r\n",
        "            range(len(self.articles)),\r\n",
        "            no_of_bands=self.bands\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "class PlagiarismDetectorImpl(PlagiarismDetector):\r\n",
        "    def __init__(self, n_gram, bands, rows):\r\n",
        "        super().__init__(n_gram, bands, rows)\r\n",
        "\r\n",
        "    def preprocess_article(self, article: str):\r\n",
        "        a = article.lower()  # lower case\r\n",
        "        a = a.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\", \"\")  # rewrite contractions\r\n",
        "        a = re.sub(\" [^ ]*&amp[^ ]*\", \"\", a)  # remove random \"&amp\" in text\r\n",
        "        a = a.translate(str.maketrans('', '', string.digits))  # remove numbers?\r\n",
        "        a = re.sub(\" +\", \" \", a)  # remove double spaces\r\n",
        "        a = a.translate(str.maketrans('', '', string.punctuation))  # remove ALL punctuation\r\n",
        "        return a\r\n",
        "\r\n",
        "\r\n",
        "    def construct_M(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def get_hash_function(self, seed:int):\r\n",
        "        random.seed(seed)\r\n",
        "        seeds = [random.getrandbits(64) for i in range(self.signature_length)]\r\n",
        "        return [lambda shingle:crc64(s + shingle) for s in seeds]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_B70lpvJWF"
      },
      "source": [
        "Read the data of the small news article set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vU3FzItcJpb"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv', index_col=0)\r\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFzLFWmykTIf"
      },
      "source": [
        "All the articles in the small article dataset will be processed to a list of the terms in the articles. The words are lowercased and duplicates are removed by using a set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-pvBwR-ij75"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "    temp = (row['article'].lower().split())\r\n",
        "    temp = set(temp)\r\n",
        "    articleList.append(temp)\r\n",
        "    \r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZ5tTpEvSrv"
      },
      "source": [
        "Calculate the jaccard index between each two documents in the data set by dividing the length of the intersection with the length of the union of the two sets. Save the values to a list to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrEpQqXnT_1"
      },
      "source": [
        "jaccardVals = []\r\n",
        "\r\n",
        "for doc1idx in range(len(articleList)):\r\n",
        "  doc1 = articleList[doc1idx]\r\n",
        "  doc2idx = doc1idx + 1\r\n",
        "  while doc2idx < len(articleList):\r\n",
        "    doc2 = articleList[doc2idx]\r\n",
        "    jaccard = len(doc1.intersection(doc2)) / len(doc1.union(doc2))\r\n",
        "    jaccardVals.append(jaccard)\r\n",
        "    doc2idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11mRB8VGvpo_"
      },
      "source": [
        "Plot the amount of values per bin, using a total of 50 bins.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v329XPSZrypo"
      },
      "source": [
        "jaccardVals = np.array(jaccardVals)\r\n",
        "sns.histplot(jaccardVals, bins=50)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrn3fKmv2YQ"
      },
      "source": [
        "The previous graph showed a peak in a small range of the possible similarities. To see the distribution in other ranges, we leave the peak values out.\r\n",
        "\r\n",
        "From this it is clear that there are also values in the higher ranges, however there are not a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPBpOnk4uifi"
      },
      "source": [
        "sns.histplot(jaccardVals[jaccardVals>0.2], bins=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18E27Bkpq2I"
      },
      "source": [
        "# 2. Preprocessing of data, shingling, and minhashing to generate a signature matrix using news articles small.csv dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs629Cm2qJua"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Qiy4dFqQIW"
      },
      "source": [
        "get content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JgW0F_cqIX3"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "  #News_ID = int(row['News_ID']) # id\r\n",
        "  article = row['article'] # lower case\r\n",
        "  #article = article.lower() # lower case\r\n",
        "  #article = article.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\",\"\") # rewrite contractions\r\n",
        "  #article = re.sub(\" [^ ]*&amp[^ ]*\",\"\", article) # remove random \"&amp\"'s in text\r\n",
        "  #article = article.translate(str.maketrans('', '', string.digits)) # remove numbers?\r\n",
        "  #article = re.sub(\" +\",\" \", article) # remove double spaces\r\n",
        "  #article = article.translate(str.maketrans('', '', string.punctuation)) # remove ALL punctuation\r\n",
        "  articleList.append(article)\r\n",
        "\r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1rHdpk_GcsA"
      },
      "source": [
        "N_GRAM = 3\r\n",
        "M_LENGTH = 40  # permutations/hash functions\r\n",
        "BANDS = 10\r\n",
        "print(\"Rows/band =\", int(M_LENGTH/BANDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1ecBFFvrdo"
      },
      "source": [
        "# Create MinHash object.\r\n",
        "minhash = MinHash(articleList, n_gram=N_GRAM, n_gram_type='term', permutations=M_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Ji97-829Ot"
      },
      "source": [
        "# Create LSH model.\r\n",
        "lsh = LSH(minhash, range(len(articleList)), no_of_bands=BANDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ly0h75s3Y4H"
      },
      "source": [
        "results = lsh.edge_list(min_jaccard=0.7, jaccard_weighted=True)\r\n",
        "\r\n",
        "print(len(results), \"near duplicates found\")\r\n",
        "print(\"DOC1\", \"DOC2\", \"JACCARD\")\r\n",
        "for doc1_id,doc2_id,jaccardVal in results:\r\n",
        "  print(doc1_id ,\"\",doc2_id, \"\", jaccardVal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFTOtqMLFya5"
      },
      "source": [
        "# test doc contains 3 sentences from docs 0, 1 and 2\r\n",
        "plagiarism_doc=\"Jorge Sosa won for the sixth time as the New York Mets snapped a four-game losing streak with a 3-0 victory over Detroit on Friday night. Sinn Fein, the Irish Republican Army's political wing, has no place in Northern Ireland politics, US Senator Ted Kennedy said Tuesday, explaining his refusal to meet this week with Gerry Adams, the group's leader. As awful as the news of priests forcing sex on altar boys is, to many of the faithful who sit in a pew each Sunday, the reaction of Roman Catholic Church leaders is even more shocking.\"\r\n",
        "new_minhash = MinHash([plagiarism_doc], n_gram=N_GRAM, n_gram_type='term', permutations=M_LENGTH)\r\n",
        "lsh.update(new_minhash, [\"plagiarized_doc\"])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9sBkEEtLJo5"
      },
      "source": [
        "results = lsh.edge_list(min_jaccard=0.4, jaccard_weighted=True)\r\n",
        "\r\n",
        "print(len(results), \"near duplicates found\")\r\n",
        "print(\"DOC1\", \"DOC2\", \"JACCARD\")\r\n",
        "for doc1_id,doc2_id,jaccardVal in results:\r\n",
        "  print(doc1_id ,\"\",doc2_id, \"\", jaccardVal)\r\n",
        "\r\n",
        "print(lsh.contains())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTVjK7QKto0"
      },
      "source": [
        ""
      ]
    }
  ]
}