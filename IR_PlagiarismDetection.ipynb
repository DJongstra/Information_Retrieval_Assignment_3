{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-PlagiarismDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJongstra/Information_Retrieval_Assignment_3/blob/main/IR_PlagiarismDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXxn7diu_iP"
      },
      "source": [
        "## Setup\r\n",
        "- Import all needed libraries\r\n",
        "- Google Drive mount\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLlGonAdb7yF"
      },
      "source": [
        "!pip install mmh3\r\n",
        "!pip install snapy\r\n",
        "!pip install xxhash\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import string, re, random, xxhash\r\n",
        "from snapy import MinHash, LSH\r\n",
        "from google.cloud import storage\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv6-qQBwgnDs"
      },
      "source": [
        "## LSH functionality (super class)\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK59oJ_jgKIt"
      },
      "source": [
        "class LSHFunctionality:\r\n",
        "    def __init__(self, n_gram, bands, rows, seed):\r\n",
        "        self.n_gram = n_gram\r\n",
        "        self.bands = bands\r\n",
        "        self.rows = rows\r\n",
        "        self.seed = seed\r\n",
        "        self.signature_length = bands * rows\r\n",
        "        self.original_documents = []\r\n",
        "\r\n",
        "    # read directly from csv file\r\n",
        "    def read_csv(self, csv_file):\r\n",
        "        for _, row in csv_file.iterrows():\r\n",
        "            self.original_documents.append(row['article'])\r\n",
        "\r\n",
        "    # add documents as a list of strings\r\n",
        "    def add_documents(self, documents: []):\r\n",
        "        self.original_documents = documents\r\n",
        "\r\n",
        "    # after adding documents, call compute to start the LSH\r\n",
        "    def compute(self):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # return all similarities >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # compare all docs to 'content' and return all where >= s\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xvVugxbzu0n"
      },
      "source": [
        "## LSH using a library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frkv-5bKz4Ee"
      },
      "source": [
        "# We first start with implementing this functionality with the library\r\n",
        "# https://pypi.org/project/snapy/\r\n",
        "class LSHLibrary(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed=123):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.lsh = None\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        self.lsh = LSH(\r\n",
        "            MinHash(\r\n",
        "                self.original_documents,\r\n",
        "                n_gram=self.n_gram,\r\n",
        "                n_gram_type='term',\r\n",
        "                permutations=self.signature_length,\r\n",
        "                seed=self.seed\r\n",
        "            ),\r\n",
        "            range(len(self.original_documents)),\r\n",
        "            no_of_bands=self.bands\r\n",
        "        )\r\n",
        "\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        return self.lsh.edge_list(min_jaccard=s, jaccard_weighted=True)\r\n",
        "\r\n",
        "    # to query some content, we first have to add it to our set, minhash it and than query its id..\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        doc_id = len(self.original_documents)\r\n",
        "        self.original_documents.append(content)\r\n",
        "\r\n",
        "        # add to set (M)\r\n",
        "        self.lsh.update(MinHash(\r\n",
        "            [content],\r\n",
        "            n_gram=self.n_gram,\r\n",
        "            n_gram_type='term',\r\n",
        "            permutations=self.signature_length,\r\n",
        "            seed=self.seed\r\n",
        "        ), [doc_id])\r\n",
        "\r\n",
        "        # query matching documents\r\n",
        "        return self.lsh.query(doc_id, min_jaccard=s)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbWLY700KaC"
      },
      "source": [
        "## Our own LSH implementation\r\n",
        "\r\n",
        "\r\n",
        "### Hash function\r\n",
        "The class HashFunction uses the **xxhash** library to hash to following data:\r\n",
        "*   Shingles (list of strings) to 64 bit integers\r\n",
        "*   The previously hashed shingles (64 bit) from sketches. We use a fixed size key to create different hash functions h_0 to h_|M|. Even before reading our documents, we will generate a list of these hash functions based on a **seed**.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAEW9rie0a_Q"
      },
      "source": [
        "# Hash function object that can be prepared\r\n",
        "# If no key is provided this function will be a normal xxhash\r\n",
        "# If two hash functions share the same key, they will also generate the same output for a given input\r\n",
        "class HashFunction:\r\n",
        "    def __init__(self, key: int = None):\r\n",
        "        self.key = self.int_to_bytes(key) if key else b''  # store key\r\n",
        "\r\n",
        "    # [\"rose\", \"is\", \"a\"] -> 189939623769124324\r\n",
        "    def compute_strings(self, shingle: []):\r\n",
        "        h = xxhash.xxh64()\r\n",
        "        for word in shingle:\r\n",
        "            h.update(word)\r\n",
        "        return self.to_64_bit(h.digest())\r\n",
        "\r\n",
        "    # (hashed shingle) 189939623769124324 ->  (rank) 134237347983861913\r\n",
        "    def compute_int64(self, shingle: int):\r\n",
        "        h = xxhash.xxh64(self.key)\r\n",
        "        h.update(self.int_to_bytes(shingle))\r\n",
        "        return self.to_64_bit(h.digest())\r\n",
        "\r\n",
        "    # convert 64 bit integer to 8 bytes\r\n",
        "    def int_to_bytes(self, i: int):\r\n",
        "        return int.to_bytes(i, length=8, byteorder='big', signed=False)\r\n",
        "\r\n",
        "    # convert 16 byte hash digest (128 bit) to a 64 bit integer (8 bytes)\r\n",
        "    def to_64_bit(self, digest: bytes):\r\n",
        "        return int.from_bytes(digest[:8], byteorder='big', signed=False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-WygPQc5A8z"
      },
      "source": [
        "## LSH class\r\n",
        "\r\n",
        "After all documents are added, the set is static and no documents can be added after. We could implement this, but we won't need it for this assignment.\r\n",
        "We can however compare new content to our existing set using query_content(doc, s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zhT_B3s5gpK"
      },
      "source": [
        "class LSHImplementation(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed=123):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.hash_tables = []  # one dictionary for each band\r\n",
        "        self.M = []  # A signature vector for each document\r\n",
        "        #  document pairs are keys, and the values are the amount of bands the documents correspond in.\r\n",
        "        # if a doc pair is not present in the dictionary, then theur similarity is 0\r\n",
        "        self.similarities = {}  \r\n",
        "        # prepare signature hash functions based on a seed\r\n",
        "        random.seed(seed)\r\n",
        "        self.prepared_hash_functions = [HashFunction(key=random.getrandbits(64)) for _ in range(self.signature_length)]\r\n",
        "\r\n",
        "    # String \"We don't need to use a library, great!\" -> \"we do not need to use a library great\"\r\n",
        "    def preprocess_document(self, document: str):\r\n",
        "        doc = document.lower()  # lower case\r\n",
        "        doc = doc.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\", \"\")  # rewrite contractions\r\n",
        "        doc = re.sub(\" [^ ]*&amp[^ ]*\", \"\", doc)  # remove random \"&amp\" in text\r\n",
        "        doc = doc.translate(str.maketrans('', '', string.digits))  # remove numbers?\r\n",
        "        doc = re.sub(\" +\", \" \", doc)  # remove double spaces\r\n",
        "        doc = doc.translate(str.maketrans('', '', string.punctuation))  # remove ALL punctuation\r\n",
        "        return doc\r\n",
        "\r\n",
        "    # -> \"rose is a rose is a rose\"\r\n",
        "    # -> [[\"rose\", \"is\", \"a\"], [\"is\", \"a\", \"rose\"], [\"a\", \"rose\", \"is\"], [\"rose\", \"is\", \"a\"], [\"is\", \"a\", \"rose\"]]\r\n",
        "    # -> [44, 24, 17, 44, 24]\r\n",
        "    # -> {44, 24, 17} use set to remove duplicates\r\n",
        "    def doc_to_hashed_shingles(self, doc):\r\n",
        "        terms = doc.split()\r\n",
        "        hash_f = HashFunction()  # key=None\r\n",
        "        no_shingles = len(terms) - self.n_gram + 1\r\n",
        "        return set([hash_f.compute_strings(terms[i:i + self.n_gram]) for i in range(no_shingles)])\r\n",
        "\r\n",
        "    # Pre process the document, shingle its contents, hash the shingles and create the signature using minhash\r\n",
        "    def doc_to_signature(self, original_doc):\r\n",
        "        # \"rose is a rose is a rose\"\r\n",
        "        doc = self.preprocess_document(original_doc)\r\n",
        "        # To set of shingles: {34, 727, 1, .., 934}\r\n",
        "        hashed_shingles = self.doc_to_hashed_shingles(doc)\r\n",
        "        signature = []\r\n",
        "        for hash_f in self.prepared_hash_functions:\r\n",
        "            # returns shingle for which h_i outputs the minimum value\r\n",
        "            min_hash = min(hashed_shingles, key=hash_f.compute_int64)\r\n",
        "            signature.append(min_hash)\r\n",
        "        return signature  # <- we got our sketch!\r\n",
        "\r\n",
        "    # Construct M, create Hash Tables and get Similarities\r\n",
        "    def compute(self):\r\n",
        "        print(\"Construct M...\")\r\n",
        "        self.M = self.construct_M()\r\n",
        "        print(\"Construct hash tables...\")\r\n",
        "        self.hash_tables = self.construct_hash_tables()\r\n",
        "        print(\"Construct similarities...\")\r\n",
        "        self.similarities = self.construct_similarities()\r\n",
        "\r\n",
        "    def construct_M(self):\r\n",
        "        M = []\r\n",
        "        for original_doc in self.original_documents:\r\n",
        "            signature = self.doc_to_signature(original_doc)\r\n",
        "            M.append(signature)\r\n",
        "        return M\r\n",
        "\r\n",
        "    # Construct a hash table (dictionary) for each band, the row values in the signature is a key in the table\r\n",
        "    # If doc1 has values (1,2,3) for band 2, and doc2 also has values (1,2,3) for band 2,\r\n",
        "    # then they will end up in the same bucket.\r\n",
        "    def construct_hash_tables(self):\r\n",
        "        loading = LoadingBar(loops=self.bands*len(self.M))  # ignore\r\n",
        "\r\n",
        "        bands_hash_tables = []\r\n",
        "        for b in range(self.bands):\r\n",
        "            hash_table = {}\r\n",
        "            for doc_id in range(len(self.M)):\r\n",
        "                signature = self.M[doc_id]\r\n",
        "                key = tuple(signature[b * self.rows:(b + 1) * self.rows])\r\n",
        "                if key in hash_table:\r\n",
        "                    hash_table[key].append(doc_id)\r\n",
        "                else:\r\n",
        "                    hash_table[key] = [doc_id]\r\n",
        "                loading.next()  # ignore\r\n",
        "            bands_hash_tables.append(hash_table)\r\n",
        "        return bands_hash_tables\r\n",
        "\r\n",
        "    # Construct all similarities by keeping track of all hits between documents\r\n",
        "    # Result -> {(doc1, doc2):5, (doc2, doc7):3}\r\n",
        "    # If total_bands=10, then the jaccard for doc1&2 is 5/10 = 0.5\r\n",
        "    def construct_similarities(self):\r\n",
        "        loading = LoadingBar(loops=self.bands)  # ignore\r\n",
        "\r\n",
        "        similarities = {}\r\n",
        "        for b in range(self.bands):\r\n",
        "            for sim_list in self.hash_tables[b].values():\r\n",
        "                no_docs = len(sim_list)\r\n",
        "                if no_docs > 1:\r\n",
        "                    for i in range(no_docs - 1):\r\n",
        "                        for j in range(i + 1, no_docs):\r\n",
        "                            key = tuple([sim_list[i], sim_list[j]])\r\n",
        "                            if key in similarities:\r\n",
        "                                similarities[key] += 1\r\n",
        "                            else:\r\n",
        "                                similarities[key] = 1\r\n",
        "            loading.next()  # ignore\r\n",
        "        return similarities\r\n",
        "\r\n",
        "    # Get all document id's where the jaccard >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        # Now the jaccard value is the amount of band hits / total_bands, but only return if >= s\r\n",
        "        return [(doc1, doc2, hits / self.bands)\r\n",
        "                for ((doc1, doc2), hits) in self.similarities.items() if hits / self.bands >= s]\r\n",
        "\r\n",
        "    # Create a signature for the new document, and compare its bands with the bands hash table to find similar documents\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        similarities = {}\r\n",
        "        signature = self.doc_to_signature(content)\r\n",
        "        for b in range(self.bands):\r\n",
        "            key = tuple(signature[b * self.rows:(b + 1) * self.rows])\r\n",
        "            if key in self.hash_tables[b]:\r\n",
        "                # all documents that share the same row values in band b\r\n",
        "                for doc_id in self.hash_tables[b][key]:\r\n",
        "                    # keep counters how many times another doc has the same band values\r\n",
        "                    if doc_id in similarities:\r\n",
        "                        similarities[doc_id] += 1\r\n",
        "                    else:\r\n",
        "                        similarities[doc_id] = 1\r\n",
        "\r\n",
        "        # Now the jaccard value is the amount of band hits / total_bands, but only return if >= s\r\n",
        "        return [(doc, hits / self.bands)\r\n",
        "                for (doc, hits) in similarities.items() if hits / self.bands >= s]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_B70lpvJWF"
      },
      "source": [
        "Read the data of the small news article set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vU3FzItcJpb"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv', index_col=0)\r\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFzLFWmykTIf"
      },
      "source": [
        "All the articles in the small article dataset will be processed to a list of the terms in the articles. The words are lowercased and duplicates are removed by using a set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-pvBwR-ij75"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "    temp = (row['article'].lower().split())\r\n",
        "    temp = set(temp)\r\n",
        "    articleList.append(temp)\r\n",
        "    \r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZ5tTpEvSrv"
      },
      "source": [
        "Calculate the jaccard index between each two documents in the data set by dividing the length of the intersection with the length of the union of the two sets. Save the values to a list to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrEpQqXnT_1"
      },
      "source": [
        "jaccardVals = []\r\n",
        "\r\n",
        "for doc1idx in range(len(articleList)):\r\n",
        "  doc1 = articleList[doc1idx]\r\n",
        "  doc2idx = doc1idx + 1\r\n",
        "  while doc2idx < len(articleList):\r\n",
        "    doc2 = articleList[doc2idx]\r\n",
        "    jaccard = len(doc1.intersection(doc2)) / len(doc1.union(doc2))\r\n",
        "    jaccardVals.append(jaccard)\r\n",
        "    doc2idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11mRB8VGvpo_"
      },
      "source": [
        "Plot the amount of values per bin, using a total of 50 bins.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v329XPSZrypo"
      },
      "source": [
        "jaccardVals = np.array(jaccardVals)\r\n",
        "sns.histplot(jaccardVals, bins=50)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrn3fKmv2YQ"
      },
      "source": [
        "The previous graph showed a peak in a small range of the possible similarities. To see the distribution in other ranges, we leave the peak values out.\r\n",
        "\r\n",
        "From this it is clear that there are also values in the higher ranges, however there are not a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPBpOnk4uifi"
      },
      "source": [
        "sns.histplot(jaccardVals[jaccardVals>0.2], bins=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18E27Bkpq2I"
      },
      "source": [
        "# 2. Preprocessing of data, shingling, and minhashing to generate a signature matrix using news articles small.csv dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs629Cm2qJua"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Qiy4dFqQIW"
      },
      "source": [
        "get content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JgW0F_cqIX3"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "  #News_ID = int(row['News_ID']) # id\r\n",
        "  article = row['article'] # lower case\r\n",
        "  #article = article.lower() # lower case\r\n",
        "  #article = article.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\",\"\") # rewrite contractions\r\n",
        "  #article = re.sub(\" [^ ]*&amp[^ ]*\",\"\", article) # remove random \"&amp\"'s in text\r\n",
        "  #article = article.translate(str.maketrans('', '', string.digits)) # remove numbers?\r\n",
        "  #article = re.sub(\" +\",\" \", article) # remove double spaces\r\n",
        "  #article = article.translate(str.maketrans('', '', string.punctuation)) # remove ALL punctuation\r\n",
        "  articleList.append(article)\r\n",
        "\r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1rHdpk_GcsA"
      },
      "source": [
        "N_GRAM = 3\r\n",
        "M_LENGTH = 40  # permutations/hash functions\r\n",
        "BANDS = 10\r\n",
        "print(\"Rows/band =\", int(M_LENGTH/BANDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1ecBFFvrdo"
      },
      "source": [
        "# Create MinHash object.\r\n",
        "minhash = MinHash(articleList, n_gram=N_GRAM, n_gram_type='term', permutations=M_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Ji97-829Ot"
      },
      "source": [
        "# Create LSH model.\r\n",
        "lsh = LSH(minhash, range(len(articleList)), no_of_bands=BANDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ly0h75s3Y4H"
      },
      "source": [
        "results = lsh.edge_list(min_jaccard=0.7, jaccard_weighted=True)\r\n",
        "\r\n",
        "print(len(results), \"near duplicates found\")\r\n",
        "print(\"DOC1\", \"DOC2\", \"JACCARD\")\r\n",
        "for doc1_id,doc2_id,jaccardVal in results:\r\n",
        "  print(doc1_id ,\"\",doc2_id, \"\", jaccardVal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFTOtqMLFya5"
      },
      "source": [
        "# test doc contains 3 sentences from docs 0, 1 and 2\r\n",
        "plagiarism_doc=\"Jorge Sosa won for the sixth time as the New York Mets snapped a four-game losing streak with a 3-0 victory over Detroit on Friday night. Sinn Fein, the Irish Republican Army's political wing, has no place in Northern Ireland politics, US Senator Ted Kennedy said Tuesday, explaining his refusal to meet this week with Gerry Adams, the group's leader. As awful as the news of priests forcing sex on altar boys is, to many of the faithful who sit in a pew each Sunday, the reaction of Roman Catholic Church leaders is even more shocking.\"\r\n",
        "new_minhash = MinHash([plagiarism_doc], n_gram=N_GRAM, n_gram_type='term', permutations=M_LENGTH)\r\n",
        "lsh.update(new_minhash, [\"plagiarized_doc\"])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9sBkEEtLJo5"
      },
      "source": [
        "results = lsh.edge_list(min_jaccard=0.4, jaccard_weighted=True)\r\n",
        "\r\n",
        "print(len(results), \"near duplicates found\")\r\n",
        "print(\"DOC1\", \"DOC2\", \"JACCARD\")\r\n",
        "for doc1_id,doc2_id,jaccardVal in results:\r\n",
        "  print(doc1_id ,\"\",doc2_id, \"\", jaccardVal)\r\n",
        "\r\n",
        "print(lsh.contains())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTVjK7QKto0"
      },
      "source": [
        ""
      ]
    }
  ]
}