{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-PlagiarismDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJongstra/Information_Retrieval_Assignment_3/blob/main/IR_PlagiarismDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXxn7diu_iP"
      },
      "source": [
        "# 1. Setup\r\n",
        "- Import all needed libraries\r\n",
        "- Google Drive mount\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLlGonAdb7yF"
      },
      "source": [
        "from google.cloud import storage\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!pip install mmh3\r\n",
        "!pip install snapy\r\n",
        "!pip install xxhash\r\n",
        "!pip install Random-Word-Generator\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import string, re, random, xxhash, time\r\n",
        "from snapy import MinHash, LSH\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_B70lpvJWF"
      },
      "source": [
        "# 2. Similarity Analysis: Ground Truth\r\n",
        "Preprocessing of a document\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZ0T12hHiEV"
      },
      "source": [
        "# \"We don't need to use a library, great!\" -> [\"we\", \"do\", \"not\", \"need\", \"to\", \"use\", \"a\", \"library\", \"great\"]\r\n",
        "def preprocess_document(document: str):\r\n",
        "    doc = document.lower()  # lower case\r\n",
        "    doc = doc.replace(\"n't\", \" not\").replace(\"'ve\", \" have\").replace(\"'s\", \"\")  # rewrite contractions\r\n",
        "    doc = re.sub(\" [^ ]*&amp[^ ]*\", \"\", doc)  # remove random \"&amp\" in text\r\n",
        "    doc = doc.translate(str.maketrans('', '', string.digits))  # remove numbers?\r\n",
        "    doc = re.sub(\" +\", \" \", doc)  # remove double spaces\r\n",
        "    doc = doc.translate(str.maketrans('', '', string.punctuation))  # remove ALL punctuation\r\n",
        "    return doc.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJQtjrXJb4t"
      },
      "source": [
        "Load the small article dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vU3FzItcJpb"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv', index_col=0)\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU1BIa83fbwe"
      },
      "source": [
        "df['article'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFzLFWmykTIf"
      },
      "source": [
        "All the articles in the small article dataset will be processed to a list of the terms in the articles. The words are lowercased and duplicates are removed by using a set (because order does not matter in this part of the analysis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-pvBwR-ij75"
      },
      "source": [
        "articleList = []\r\n",
        "\r\n",
        "for _, row in df.iterrows():\r\n",
        "    terms = preprocess_document(row['article'])\r\n",
        "    articleList.append(set(terms))\r\n",
        "    \r\n",
        "print(articleList[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UZ5tTpEvSrv"
      },
      "source": [
        "Calculate the jaccard index between each two documents in the data set by dividing the length of the intersection with the length of the union of the two sets. Save the values to a list to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrEpQqXnT_1"
      },
      "source": [
        "jaccardVals = []\r\n",
        "\r\n",
        "for doc1idx in range(len(articleList)):\r\n",
        "  doc1 = articleList[doc1idx]\r\n",
        "  doc2idx = doc1idx + 1\r\n",
        "  while doc2idx < len(articleList):\r\n",
        "    doc2 = articleList[doc2idx]\r\n",
        "    jaccard = len(doc1.intersection(doc2)) / len(doc1.union(doc2))\r\n",
        "    jaccardVals.append(jaccard)\r\n",
        "    doc2idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11mRB8VGvpo_"
      },
      "source": [
        "Plot the amount of values per bin, using a total of 50 bins.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v329XPSZrypo"
      },
      "source": [
        "jaccardVals = np.array(jaccardVals)\r\n",
        "sns.histplot(jaccardVals, bins=50)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGrn3fKmv2YQ"
      },
      "source": [
        "The previous graph showed a peak in a small range of the possible similarities. To see the distribution in other ranges, we leave the peak values out.\r\n",
        "\r\n",
        "From this it is clear that there are also values in the higher ranges, however there are not a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPBpOnk4uifi"
      },
      "source": [
        "sns.histplot(jaccardVals[jaccardVals>0.2], bins=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbWLY700KaC"
      },
      "source": [
        "# 3. LSH Implementation\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJbKpQtYWeVq"
      },
      "source": [
        "## 3.1 Hash functions\r\n",
        "The class RankHash uses the **xxhash** library to hash previously hashed shingles (64 bit) from sketches. We use a fixed size deterministic salt to create different hash functions h_0 to h_|M|. Even before reading our documents, we will generate a list of these hash functions based on a **seed**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAEW9rie0a_Q"
      },
      "source": [
        "# convert integer to 8 bytes\r\n",
        "def to_bytes(i: int):\r\n",
        "    return int.to_bytes(i, length=8, byteorder='big', signed=False)\r\n",
        "\r\n",
        "# convert hash digest (16 bytes) to an X byte integer\r\n",
        "def to_int(digest: bytes, no_bytes=None):\r\n",
        "    return int.from_bytes(digest[:no_bytes] if no_bytes else digest, byteorder='big', signed=False)\r\n",
        "\r\n",
        "# [\"rose\", \"is\", \"a\"] -> 189939623769124324 (x bytes)\r\n",
        "def hash_shingle(shingle: [], no_bytes=8):\r\n",
        "    xxh = xxhash.xxh64()\r\n",
        "    for word in shingle:\r\n",
        "        xxh.update(word)\r\n",
        "    return to_int(xxh.digest(), no_bytes)\r\n",
        "\r\n",
        "# Hash function object that can be prepared\r\n",
        "# If two objects share the same salt, they will also generate the same output for a given input\r\n",
        "class RankHash:\r\n",
        "    def __init__(self, salt: int):\r\n",
        "        self.salt = to_bytes(salt)  # store key/salt\r\n",
        "\r\n",
        "    # (hashed shingle) 189939623769124324 -> (rank) 134237347983861913\r\n",
        "    def rank(self, hashed_shingle: int):\r\n",
        "        xxh = xxhash.xxh64()\r\n",
        "        xxh.update(self.salt)\r\n",
        "        xxh.update(to_bytes(hashed_shingle))\r\n",
        "        return to_int(xxh.digest())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-WygPQc5A8z"
      },
      "source": [
        "## 3.2 LSH functionality\r\n",
        "\r\n",
        "After all documents are added, the set is static and no documents can be added after. However, we can compare new content to our existing set using query_content(doc, s).\r\n",
        "We could implement insertion, but we won't need it for this assignment.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycDcFOrfV5_v"
      },
      "source": [
        "# Basic functionality that the library as wel as our own implementation must handle\r\n",
        "class LSHFunctionality:\r\n",
        "    def __init__(self, n_gram, bands, rows, seed):\r\n",
        "        self.n_gram = n_gram\r\n",
        "        self.bands = bands\r\n",
        "        self.rows = rows\r\n",
        "        self.seed = seed\r\n",
        "        self.signature_length = bands * rows\r\n",
        "        self.original_documents = []\r\n",
        "\r\n",
        "    # read directly from csv file\r\n",
        "    def read_csv(self, csv_file: str):\r\n",
        "        for _, row in pd.read_csv(csv_file, index_col=0).iterrows():\r\n",
        "            self.original_documents.append(row['article'])\r\n",
        "\r\n",
        "\r\n",
        "    # add documents as a list of strings\r\n",
        "    def add_documents(self, documents: []):\r\n",
        "        self.original_documents = documents\r\n",
        "\r\n",
        "    # after adding documents, call compute to start the LSH\r\n",
        "    def compute(self):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # return all similarities >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n",
        "\r\n",
        "    # compare all docs to 'content' and return all where >= s\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        raise Exception(\"virtual\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgkYp2pEV-iM"
      },
      "source": [
        "## 3.3 Using a Library\r\n",
        "We first start with implementing this functionality with the library\r\n",
        "\r\n",
        "https://pypi.org/project/snapy/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTSjafm5WQS8"
      },
      "source": [
        "class LSHLibrary(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.lsh = None\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        self.lsh = LSH(\r\n",
        "            MinHash(\r\n",
        "                self.original_documents,\r\n",
        "                n_gram=self.n_gram,\r\n",
        "                n_gram_type='term',\r\n",
        "                permutations=self.signature_length,\r\n",
        "                seed=self.seed\r\n",
        "            ),\r\n",
        "            range(len(self.original_documents)),\r\n",
        "            no_of_bands=self.bands\r\n",
        "        )\r\n",
        "\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        return self.lsh.edge_list(min_jaccard=s, jaccard_weighted=True)\r\n",
        "\r\n",
        "    # to query some content, we first have to add it to our set, minhash it and than query its id..\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        doc_id = len(self.original_documents)\r\n",
        "        self.original_documents.append(content)\r\n",
        "\r\n",
        "        # add to set (M)\r\n",
        "        self.lsh.update(MinHash(\r\n",
        "            [content],\r\n",
        "            n_gram=self.n_gram,\r\n",
        "            n_gram_type='term',\r\n",
        "            permutations=self.signature_length,\r\n",
        "            seed=self.seed\r\n",
        "        ), [doc_id])\r\n",
        "\r\n",
        "        # query matching documents\r\n",
        "        return self.lsh.query(doc_id, min_jaccard=s)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTmCAruWsm_"
      },
      "source": [
        "## 3.4 Our own implementation of LSH\r\n",
        "Our own implementation requires some additional methods to get all the functionality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zhT_B3s5gpK"
      },
      "source": [
        "class LSHImplementation(LSHFunctionality):\r\n",
        "    def __init__(self, n_gram, bands, rows, seed=123):\r\n",
        "        super().__init__(n_gram, bands, rows, seed)\r\n",
        "        self.M = []  # a signature matrix for each document\r\n",
        "        self.buckets = {}  # a dictionary with buckets\r\n",
        "        self.similarities = {}  # document pairs are keys and the values are band hits\r\n",
        "        random.seed(seed)  # prepare signature hash functions based on seed\r\n",
        "        self.prepared_hash_functions = [RankHash(salt=random.getrandbits(64)) for _ in range(self.signature_length)]\r\n",
        "\r\n",
        "    # Construct M, create buckets and compute similarities\r\n",
        "    def compute(self):\r\n",
        "        # Create a signature for each document\r\n",
        "        self.M = [self.doc_to_signature(original_doc) for original_doc in self.original_documents]\r\n",
        "        self.buckets = self.construct_buckets()\r\n",
        "        self.similarities = self.construct_similarities()\r\n",
        "\r\n",
        "    # Pre process the document, shingle its contents, hash the shingles and create the signature using minhash\r\n",
        "    def doc_to_signature(self, original_doc):\r\n",
        "        # Returns [\"rose\", \"is\", \"a\", \"rose\", \"is\", \"a\", \"rose\"]\r\n",
        "        terms = preprocess_document(original_doc)\r\n",
        "        # hash all the shingles\r\n",
        "        hashed_shingles = set()\r\n",
        "        for i in range(len(terms) - self.n_gram + 1):\r\n",
        "            shingle = terms[i:i + self.n_gram]  # [\"rose\", \"is\", \"a\"]\r\n",
        "            h = hash_shingle(shingle)  # 14164490265723533732547384763 (hash)\r\n",
        "            hashed_shingles.add(h)\r\n",
        "        # compute the minhash for every prepared ranking function (=signature length)\r\n",
        "        return [min(hashed_shingles, key=h_r.rank) for h_r in self.prepared_hash_functions]  # <- sketch!\r\n",
        "\r\n",
        "    # Construct a buckets(dictionary)\r\n",
        "    # If doc1 has values (1,2,3) for a band, and doc2 also has values (1,2,3) for a band,\r\n",
        "    # then they will end up in the same bucket.\r\n",
        "    def construct_buckets(self):\r\n",
        "        buckets = {}\r\n",
        "        for doc_id in range(len(self.M)):\r\n",
        "            signature = self.M[doc_id]\r\n",
        "            for band in range(self.bands):\r\n",
        "                # split signature into bands and use as key to bucket\r\n",
        "                key = tuple(signature[band * self.rows:(band + 1) * self.rows])\r\n",
        "                if key in buckets:\r\n",
        "                    buckets[key].add(doc_id)\r\n",
        "                else:\r\n",
        "                    buckets[key] = {doc_id}\r\n",
        "        return buckets\r\n",
        "\r\n",
        "    # Construct all similarities by keeping track of all hits between documents\r\n",
        "    # Result -> {(doc1, doc2):0.5, (doc2, doc7):0.3}\r\n",
        "    def construct_similarities(self):\r\n",
        "        candidate_pairs = set()  # set of candidate pairs\r\n",
        "        for bucket in [list(b) for b in self.buckets.values()]:\r\n",
        "            no_docs = len(bucket)\r\n",
        "            # make all combinations between documents in bucket d(d-1)/2\r\n",
        "            # need at least 2 docs in a bucket to create a candidate pair\r\n",
        "            for i in range(no_docs - 1):\r\n",
        "                for j in range(i + 1, no_docs):\r\n",
        "                    candidate_pairs.add((bucket[i], bucket[j]))\r\n",
        "        # map set of candidate pairs to dictionary\r\n",
        "        return {(doc1, doc2): self.compare_signatures(self.M[doc1], self.M[doc2]) for (doc1, doc2) in candidate_pairs}\r\n",
        "\r\n",
        "    def compare_signatures(self, sig1: [], sig2: []):\r\n",
        "        return len([True for s1, s2 in zip(sig1, sig2) if s1 == s2]) / self.signature_length\r\n",
        "\r\n",
        "    # Get all document id's where the similarity >= s\r\n",
        "    def get_all_similarities(self, s: float):\r\n",
        "        return [(doc1, doc2, round(sim, 2)) for ((doc1, doc2), sim) in self.similarities.items() if sim >= s]\r\n",
        "\r\n",
        "    # Create a signature for the new document, and compare its bands with the bands hash table to find similar documents\r\n",
        "    def query_content(self, content: str, s: float):\r\n",
        "        candidates = set()\r\n",
        "        signature = self.doc_to_signature(content)\r\n",
        "        for band in range(self.bands):\r\n",
        "            # split signature into bands and use as key to bucket\r\n",
        "            key = tuple(signature[band * self.rows:(band + 1) * self.rows])\r\n",
        "            if key in self.buckets:\r\n",
        "                # add candidates\r\n",
        "                candidates.update(self.buckets[key])\r\n",
        "\r\n",
        "        # for each candidate, calculate the actual similarity\r\n",
        "        result = []\r\n",
        "        for doc in candidates:\r\n",
        "            sim = self.compare_signatures(signature, self.M[doc])\r\n",
        "            if sim >= s:\r\n",
        "                result.append((doc, round(sim, 2)))\r\n",
        "        return result\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wf4yzEhZ9uJ"
      },
      "source": [
        "## 3.5 Simple Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YaW2HL9aZcg"
      },
      "source": [
        "# example from https://pypi.org/project/snapy/\r\n",
        "documents = [\r\n",
        "    'Jupiter is primarily composed of hydrogen and a quarter of its mass being helium',\r\n",
        "    'Jupiter moving out of the inner Solar System would have allowed the formation of inner planets.',\r\n",
        "    'A helium atom has about four times as much mass as a hydrogen atom, so the composition changes when described as the proportion of mass contributed by different atoms.',\r\n",
        "    'Jupiter is primarily composed of hydrogen and a quarter of its mass being helium',\r\n",
        "    'A helium atom has about four times as much mass as a hydrogen atom and the composition changes when described as a proportion of mass contributed by different atoms.',\r\n",
        "    'Theoretical models indicate that if Jupiter had much more mass than it does at present, it would shrink.',\r\n",
        "    'This process causes Jupiter to shrink by about 2 cm each year.',\r\n",
        "    'Jupiter is mostly composed of hydrogen with a quarter of its mass being helium',\r\n",
        "    'The Great Red Spot is large enough to accommodate Earth within its boundaries.'\r\n",
        "]\r\n",
        "# changed 'much' to 'a lot' from document 5\r\n",
        "plagiarized_doc = 'Theoretical models indicate that if Jupiter had a lot more mass than it does at present, it would shrink.'\r\n",
        "\r\n",
        "# test both implementations\r\n",
        "for constructor in [LSHImplementation, LSHLibrary]:\r\n",
        "    lsh = constructor(n_gram=2, bands=4, rows=2, seed=999)\r\n",
        "    lsh.add_documents(documents)\r\n",
        "    lsh.compute()\r\n",
        "    sim = 0.4\r\n",
        "    print(f\"\\n========== {lsh.__class__.__name__} ==========\")\r\n",
        "    print(f\"All similarities s>={sim}:\", lsh.get_all_similarities(sim))\r\n",
        "    print(f\"Find similar documents to plagiarized doc with s>={sim} (doc 5 expected):\",\r\n",
        "          lsh.query_content(plagiarized_doc, sim))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvUqEmqFdpXY"
      },
      "source": [
        "## 3.6 Time comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhrf71NEdxI6"
      },
      "source": [
        "print(df.head())\r\n",
        "\r\n",
        "for constructor in [LSHImplementation, LSHLibrary]:\r\n",
        "  lsh = constructor(n_gram=5, bands=5, rows=3, seed=17)\r\n",
        "\r\n",
        "  print(f\"\\n========== {lsh.__class__.__name__} ==========\")\r\n",
        "  print(\"Read CSV.. \", end='')\r\n",
        "  time_start = time.time()\r\n",
        "  lsh.read_csv('/content/drive/MyDrive/IR-Assignment-3/data/news_articles_small.csv')\r\n",
        "  print(f\"({round((time.time()-time_start)/60, 2)} minutes)\")\r\n",
        "\r\n",
        "  print(\"Construct M.. \", end='')\r\n",
        "  time_start = time.time()\r\n",
        "  lsh.compute()\r\n",
        "  print(f\"({round((time.time() - time_start) / 60, 2)} minutes)\")\r\n",
        "\r\n",
        "  s = 0.6\r\n",
        "  print(f\"Find all similar documents with s >= {s}\")\r\n",
        "  time_start = time.time()\r\n",
        "  sim = lsh.get_all_similarities(s=s)\r\n",
        "  print(f\"{len(sim)} similarities found ({round((time.time() - time_start) / 60, 2)} minutes): \", sim)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytzOUepjsAL"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm7K4FJUfroQ"
      },
      "source": [
        "### Prepare Some Plagiarised Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkA8j9JshPOS"
      },
      "source": [
        "from RandomWordGenerator import RandomWord\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq6OzbByrE1e"
      },
      "source": [
        "# Replace first x words by random words\n",
        "def create_plagiarised_doc_range(document, x):\n",
        "  words=doc_1.split(\" \")\n",
        "  rw = RandomWord(max_word_size = 5,\n",
        "                constant_word_size=True,\n",
        "                include_digits=False,\n",
        "                special_chars=r\"@_!#$%^&*()<>?/\\|}{~:\",\n",
        "                include_special_chars=False)\n",
        "  for word in range(0, x):\n",
        "    words[word] = rw.generate()\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Replace every xth word by a random word\n",
        "def create_plagiarised_doc_step(document, x):\n",
        "  words=doc_1.split(\" \")\n",
        "  rw = RandomWord(max_word_size = 5,\n",
        "                constant_word_size=True,\n",
        "                include_digits=False,\n",
        "                special_chars=r\"@_!#$%^&*()<>?/\\|}{~:\",\n",
        "                include_special_chars=False)\n",
        "  for word in range(0, len(words), x):\n",
        "    words[word] = rw.generate()\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Randomly sample from a normal distribution\n",
        "def create_plagiarised_doc_uniform(document, x):\n",
        "  words=doc_1.split(\" \")\n",
        "  rw = RandomWord(max_word_size = 5,\n",
        "                constant_word_size=True,\n",
        "                include_digits=False,\n",
        "                special_chars=r\"@_!#$%^&*()<>?/\\|}{~:\",\n",
        "                include_special_chars=False)\n",
        "  for word in range(0, x):\n",
        "    rand_index = int(np.random.uniform(0, len(words)))\n",
        "    words[rand_index] = rw.generate()\n",
        "  return \" \".join(words)\n",
        "\n",
        "\n",
        "def create_plagiarised_docs(document, duplicates):\n",
        "  # Add docstring\n",
        "  duplicates_dict = {}\n",
        "  for i in range(1, duplicates+1):\n",
        "    plagiarised_doc = create_plagiarised_doc_step(doc_1, i+1)\n",
        "    duplicates_dict[f\"plagiarised_doc_step_{i}\"] = plagiarised_doc\n",
        "  for i in range(1, duplicates+1):\n",
        "    plagiarised_doc = create_plagiarised_doc_range(doc_1, i*10)\n",
        "    duplicates_dict[f\"plagiarised_doc_range_{i}\"] = plagiarised_doc\n",
        "  for i in range(1, duplicates+1):\n",
        "    plagiarised_doc = create_plagiarised_doc_uniform(doc_1, i*10)\n",
        "    duplicates_dict[f\"plagiarised_doc_uniform_{i}\"] = plagiarised_doc\n",
        "  return duplicates_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SVwHYnjk981"
      },
      "source": [
        "doc_1 = df['article'].iloc[0]\n",
        "doc_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5U238EFscRC"
      },
      "source": [
        "duplicates_dict = create_plagiarised_docs(doc_1, 20)\n",
        "duplicates_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-klU77dlbAj"
      },
      "source": [
        "### Calculate Jaccard Similarity between Plagiarised Documents and Original Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOg9Usl2ucTQ"
      },
      "source": [
        "# Preprocess doc_1\n",
        "doc_1_set= set(preprocess_document(doc_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uglr7PomJkU"
      },
      "source": [
        "# Calculate Jaccard Similarity between Doc_1 and its duplicates\n",
        "jaccardVals = {}\n",
        "for key in duplicates_dict:\n",
        "  duplicate_terms = preprocess_document(duplicates_dict[key])\n",
        "  duplicate_set = set(duplicate_terms)\n",
        "  jaccard = len(doc_1_set.intersection(duplicate_set)) / len(doc_1_set.union(duplicate_set))\n",
        "  jaccardVals[key] = jaccard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G069f2lanJS8"
      },
      "source": [
        "# Jaccard similarity between duplicates and document 1:\n",
        "jaccardVals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxtdhwBqmONM"
      },
      "source": [
        "jaccardVals_arr = np.array(list(jaccardVals.values()))\n",
        "ax = sns.histplot(jaccardVals_arr, bins=10)\n",
        "ax.set(xlabel='Jaccard Similarity', ylabel='Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrfwFEx7RYC8"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSsg9nTLRXlX"
      },
      "source": [
        "match_dict = {}\n",
        "for b in range(50, 200, 50):\n",
        "  for r in range(1,5):\n",
        "    result_dict = {}\n",
        "    lsh = LSHImplementation(n_gram=2, bands=b, rows=r, seed=17)\n",
        "    lsh.add_documents([doc_1])\n",
        "    lsh.compute() \n",
        "    for key in duplicates_dict:\n",
        "      matches = lsh.query_content(duplicates_dict[key], s=0.1)\n",
        "      if matches:\n",
        "        result_dict[key] = matches\n",
        "    \n",
        "    match_dict[f\"b_{b}_r_{r}\"] = result_dict  \n",
        "  print(match_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjTqTl5JWZDh"
      },
      "source": [
        "precision_dict = {}\n",
        "for s in range(7, 10, 1):\n",
        "  result_dict = {}\n",
        "  s = float(s/10)\n",
        "  for test_key in match_dict:\n",
        "    tp_counter = 0\n",
        "    fp_counter = 0\n",
        "    for result_key in match_dict[test_key]:\n",
        "      doc, similarity_estimate = match_dict[test_key][result_key][0]\n",
        "      if similarity_estimate > s and jaccardVals[result_key] > s:\n",
        "        tp_counter += 1\n",
        "      elif similarity_estimate > s and jaccardVals[result_key] < s:\n",
        "        fp_counter += 1\n",
        "    if tp_counter != 0:\n",
        "      result_dict[test_key] = tp_counter/(fp_counter + tp_counter)\n",
        "  precision_dict[f\"s_{s}\"] = result_dict\n",
        "print(precision_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIlQJFSBfh9z"
      },
      "source": [
        "for s in precision_dict:\n",
        "  sorted_dict = {k: v for k, v in sorted(precision_dict[s].items(), key=lambda item: item[1])}\n",
        "  names = list(sorted_dict.keys())\n",
        "  values = list(sorted_dict.values())\n",
        "  fig = plt.figure(figsize=(35,4))\n",
        "  plt.title(f\"Precision of Different Bands and Rows for Similarity Threshold: {s}\")\n",
        "  plt.bar(names, values)\n",
        "  plt.xlabel('Bands and Rows')\n",
        "  plt.ylabel('Precision')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}